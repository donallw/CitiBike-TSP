{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98961e78",
   "metadata": {},
   "source": [
    "# CitiBike TSP\n",
    "**Algorithms Group 50**\\\n",
    "Donal Lowsley-Williams \\\n",
    "Joan La Rosa \\\n",
    "Isaac Lichter \\\n",
    "Weidong Gao \n",
    "\n",
    "We will be conducting the traveling salesman problem on New York City citibike data. We will be using each CitiBike station as the nodes in our graph, and represent the edge costs by the average time it takes to bike from one station to another. Edges will only include those rides in the CitiBike data that have actually occurred - in other words, if no ride in the data started at station A and ended at station B, we will not include that edge. This adds to the realism of the problem as it would be unlikely to take that edge anyways. Moreover, we will only have edges from one station to another - not one station to itself, as there are occurences of rides like this in the CitiBike data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c5479",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Will we be implementing these algorithms by hand, so we just need libraries for data loading and export, matrix manipulation functions, and some data exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12119b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa2897",
   "metadata": {},
   "source": [
    "# The Data\n",
    "We will get the data from CitiBike's openly available [System Data](https://s3.amazonaws.com/tripdata/index.html). The data we are using was the most recent as of this notebook's creation date, and was published by CitiBike on [**Oct 4th 2021, 01:26:29 pm**](https://s3.amazonaws.com/tripdata/202109-citibike-tripdata.csv.zip). This is the data of all rides taken on the CitiBike system during the month of September 2021. However, the preprocessing should work for any selection of the data from the system data, since they all should follow the same structure. \n",
    "\n",
    "Please store the data in a folder named **\"Data\"** with the name **\"citibike-data.csv\"**, like so: **\"Data/citibike-data.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "055d1a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joan/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3172: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Data/citibike-data.csv',\n",
    "                 parse_dates=['started_at','ended_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d75c5",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "In order to preprocess the data, \n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "First lets take a look at the length of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64717c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280221"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec2b40",
   "metadata": {},
   "source": [
    "Now, lets see what the column was that caused data type issues. We can see from the error above that it was column 7, which corresponds to the `end_station_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07b559ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end_station_id'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eed57a",
   "metadata": {},
   "source": [
    "In the data, we can see by looking at the csv file that these values are typically float values, so we can cast them to a numeric value. We set any values that can't be cast to a numeric value to `NaN` by setting `errors='coerce'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbdd6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['end_station_id'] = df['end_station_id'].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6632ff",
   "metadata": {},
   "source": [
    "We drop all null values. (those with `NaN`) This includes not just those with a faulty `end_station_id` value but those with errors elsewhere. We want to use only entries with full and complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d7eff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406494df",
   "metadata": {},
   "source": [
    "We can see that we dropped approx. 15K entries that had data issues or invalid end stations. This is negligible in the grand scheme of the project, given that we have about 3.2M entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "557e6d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3264732"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded74a03",
   "metadata": {},
   "source": [
    "Let's take a peek at our data and its types. We can see that many use the dtype object, which pandas uses to refer to strings with varying length. We can leave this be. We have a couple date items, which we set when reading the csv file. The remaining data is all float data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "148d27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3264732 entries, 0 to 3280220\n",
      "Data columns (total 13 columns):\n",
      "ride_id               object\n",
      "rideable_type         object\n",
      "started_at            datetime64[ns]\n",
      "ended_at              datetime64[ns]\n",
      "start_station_name    object\n",
      "start_station_id      float64\n",
      "end_station_name      object\n",
      "end_station_id        float64\n",
      "start_lat             float64\n",
      "start_lng             float64\n",
      "end_lat               float64\n",
      "end_lng               float64\n",
      "member_casual         object\n",
      "dtypes: datetime64[ns](2), float64(6), object(5)\n",
      "memory usage: 348.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f31efe",
   "metadata": {},
   "source": [
    "Let's conduct a quick sanity check to ensure the data is properly cleaned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d75d93e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['start_station_id'].unique()) == len(df['start_station_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25d628",
   "metadata": {},
   "source": [
    "Uh-oh. Lets dig into this to see what the problem is. Lets look at these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "838dfe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start station id uniques:  1488\n",
      "start station name uniques:  1493\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print('start station id uniques: ',len(df['start_station_id'].unique()))\n",
    "print('start station name uniques: ',len(df['start_station_name'].unique()))\n",
    "print(len(df['start_station_id'].unique()) == len(df['start_station_name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd19ff",
   "metadata": {},
   "source": [
    "So we see that there are more names than there are id's. Lets make a copy of the columns in the data causing an issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67efc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[['start_station_id', 'start_station_name']].copy().drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01928c1c",
   "metadata": {},
   "source": [
    "Next, we are going to pull each unique row by dropping the duplicates from this sample. This will let us examing all the unique start station id / name pairs. Next, we can count the frequency of each id occuring - since we know there are less id's than there are names. We can then sort this and view the id's that have more than one name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0f6b718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_station_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5303.08</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303.06</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5300.06</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5300.05</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8841.03</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382.07</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4488.09</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190.09</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422.04</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781.05</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  start_station_name\n",
       "start_station_id                    \n",
       "5303.08                            1\n",
       "5303.06                            1\n",
       "5300.06                            1\n",
       "5300.05                            1\n",
       "8841.03                            1\n",
       "5382.07                            2\n",
       "4488.09                            2\n",
       "5190.09                            2\n",
       "5422.04                            2\n",
       "4781.05                            2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.groupby('start_station_id').count().sort_values('start_station_name').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6783a1cd",
   "metadata": {},
   "source": [
    "Now we see the problematic values are the last five entries in the above dataframe. We can iterate over these and observe what might be the problem by converting them to raw NumPy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5fa2574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 0 ['Forsyth St\\\\t& Grand St' 'Forsyth St\\t& Grand St']\n",
      "Entry 1 ['Boerum Pl\\\\t& Pacific St' 'Boerum Pl\\t& Pacific St']\n",
      "Entry 2 ['Clinton St\\\\t& Cherry St' 'Clinton St\\t& Cherry St']\n",
      "Entry 3 ['Howard St & Lafayette St' 'Howard St & Centre St']\n",
      "Entry 4 ['Nassau St\\\\t& Duffield St' 'Nassau St\\t& Duffield St']\n"
     ]
    }
   ],
   "source": [
    "faulty_ids = np.array(\n",
    "    sample.groupby('start_station_id').count().sort_values('start_station_name').tail(5).T.columns\n",
    ")\n",
    "\n",
    "# iterate over the faulty station ids\n",
    "for idx, station_id in enumerate(faulty_ids):\n",
    "    faulty_entries = sample.loc[sample['start_station_id'] == station_id]\n",
    "    print('Entry',idx,np.array(faulty_entries['start_station_name']))\n",
    "del sample, faulty_entries, faulty_ids, idx, station_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ebf46",
   "metadata": {},
   "source": [
    "The problem for most of these appear to be some sort of string parsing issue. This is good, because it means we can ignore it and **simply focus on the station_id's**. However, we see that entry 3 has actual different names. Let us examing this in [Google Maps](https://www.google.com/maps/place/Citi+Bike:+Howard+St+%26+Lafayette+St/@40.7191002,-73.9996465,20z/data=!4m13!1m7!3m6!1s0x89c2598992b42c11:0xa58f248cc38ef667!2sLafayette+St+%26+Howard+St,+New+York,+NY+10013!3b1!8m2!3d40.7192055!4d-73.9998472!3m4!1s0x89c25989ed219c2b:0x5828092994c0b427!8m2!3d40.7191053!4d-73.9997333). We can see that this station is nestled right between those two streets, so it refers to the same station. Again, we can ignore this and **simply focus on the station_id's**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22dce72",
   "metadata": {},
   "source": [
    "![title](./lafayettecentre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f147ad8",
   "metadata": {},
   "source": [
    "Let us perform one last sanity check before we start feature engineering, and ensure that there are the same number of start stations as there are end stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28929be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['start_station_id'].unique()) == len(df['end_station_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a131f61",
   "metadata": {},
   "source": [
    "We can ensure that all the entries in start_station_id are also present in end_station_id. This way, we know by iterating only over one of these lists ensures we are iterating across all potential station_ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1ef20a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matches\n"
     ]
    }
   ],
   "source": [
    "is_complete = True\n",
    "for val in df['start_station_id'].unique():\n",
    "    if val not in df['end_station_id'].unique():\n",
    "        is_complete = False\n",
    "print('data matches') if is_complete else print('Error; data mistmatch')\n",
    "del val, is_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e9ecc",
   "metadata": {},
   "source": [
    "Looks like we are good to go. Let's pull the necesary data for our algorithms into another dataframe to keep things clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b639717",
   "metadata": {},
   "source": [
    "### Edge Costs\n",
    "Now we need to compute edge costs between each station, and store this in a cost matrix. We can do this by first simply subtracting the start time from the end time. Since these are date objects, we can convert them to raw integers representing seconds by dividing by a 1 second numpy date time value to get total seconds and then casting to int32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58d536a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_cost'] = ((df['ended_at'] - df['started_at']) / np.timedelta64(1, 's')).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf57417",
   "metadata": {},
   "source": [
    "Next up is creating our actual cost data. What we want here is a DataFrame that contains average trip cost from each start station to each end station (when they are not the same), across all rides between those two stations. We can do this by pulling out only our relevant values - `start_station_id`, `end_station_id`, and `trip_cost`, and then dropping all the entries where the start station is equivalent to the end station. Then we group them by their `start_station_id` / `end_station_id` and take an average of the remaining columns - which is just `trip_cost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "576f15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[['start_station_id', 'end_station_id', 'trip_cost']]\n",
    "dropped = new_df.drop(new_df[new_df['start_station_id'] == new_df['end_station_id']].index)\n",
    "edge_costs = dropped.groupby(by=['start_station_id','end_station_id']).mean().reset_index()\n",
    "del dropped, new_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90075c87",
   "metadata": {},
   "source": [
    "Lets take a look at our edge cost DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7c173c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>trip_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2782.02</td>\n",
       "      <td>674.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2832.03</td>\n",
       "      <td>1108.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2872.02</td>\n",
       "      <td>651.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2883.03</td>\n",
       "      <td>466.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2733.03</td>\n",
       "      <td>2912.08</td>\n",
       "      <td>692.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406129</th>\n",
       "      <td>8841.03</td>\n",
       "      <td>8782.01</td>\n",
       "      <td>1353.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406130</th>\n",
       "      <td>8841.03</td>\n",
       "      <td>8795.01</td>\n",
       "      <td>1514.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406131</th>\n",
       "      <td>8841.03</td>\n",
       "      <td>8795.03</td>\n",
       "      <td>843.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406132</th>\n",
       "      <td>8841.03</td>\n",
       "      <td>8799.01</td>\n",
       "      <td>1213.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406133</th>\n",
       "      <td>8841.03</td>\n",
       "      <td>8811.01</td>\n",
       "      <td>1116.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406134 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        start_station_id  end_station_id    trip_cost\n",
       "0                2733.03         2782.02   674.857143\n",
       "1                2733.03         2832.03  1108.142857\n",
       "2                2733.03         2872.02   651.750000\n",
       "3                2733.03         2883.03   466.666667\n",
       "4                2733.03         2912.08   692.333333\n",
       "...                  ...             ...          ...\n",
       "406129           8841.03         8782.01  1353.250000\n",
       "406130           8841.03         8795.01  1514.631579\n",
       "406131           8841.03         8795.03   843.333333\n",
       "406132           8841.03         8799.01  1213.818182\n",
       "406133           8841.03         8811.01  1116.375000\n",
       "\n",
       "[406134 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e842ba",
   "metadata": {},
   "source": [
    "As we can see, we have each `start_station_id`, each `end_station_id`, and the average `trip_cost` between those stations. We are now prepared to run our algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb2ce7",
   "metadata": {},
   "source": [
    "# The Algorithms\n",
    "Now that we have pulled and preprocessed our data, we can begin to test various algorithms against it. We will test the following algorithms:\n",
    "\n",
    "**Greedy**\n",
    "- Nearest Neighbor\n",
    "- Farthest First Traversal\n",
    "\n",
    "**?????**\n",
    "- Christofides Algorithm \n",
    "\n",
    "**K-opt Heuristic Algorithms**\n",
    "- 2-OPT\n",
    "- 3-OPT\n",
    "- Lin-Kernighan Heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbad1f6",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "Before we do anything, let's write a little function to time our algorithms and make for easy comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51981327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(fn, station_ids, distances):\n",
    "    start = time.time()\n",
    "    _, _ = fn(station_ids, distances)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "def compare_algorithms(fn_list, station_ids, distances, num_iters=5):\n",
    "    results = {}\n",
    "    for algo in fn_list:\n",
    "        t = 0\n",
    "        for _ in range(num_iters):\n",
    "            t += check_performance(algo, station_ids, distances)\n",
    "        results[algo.__name__] = t / num_iters\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a154e5",
   "metadata": {},
   "source": [
    "## Greedy Algorithms\n",
    "First, we will examine various greedy algorithms and their effectiveness on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a0841",
   "metadata": {},
   "source": [
    "### Nearest Neighbor\n",
    "\n",
    "The nearest neighbor algorithm is very simple and simply starts at a random node (or station in our example) and continues to visit unvisited nodes that are as close to the last visited node as possible .\n",
    "\n",
    "**Psuedocode:** \\\n",
    "These are the steps of the algorithm:\n",
    "\n",
    "    Initialize all vertices as unvisited.\n",
    "    Select an arbitrary vertex, set it as the current vertex u. Mark u as visited.\n",
    "    Find out the shortest edge connecting the current vertex u and an unvisited vertex v.\n",
    "    Set v as the current vertex u. Mark v as visited.\n",
    "    If all the vertices in the domain are visited, then terminate. Else, go to step 3.\n",
    "\n",
    "The sequence of the visited vertices is the output of the algorithm. \n",
    "\n",
    "**Time complexity:** Worst Case O(N^2)\n",
    "\n",
    "**Space complexity:** Worst Case O(N)\n",
    "\n",
    "[Source (wikipedia)](https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cc697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest Neighbor\n",
    "def nearest_neighbor(edge_costs : pd.DataFrame):\n",
    "    cost = 0\n",
    "    path = []\n",
    "    \n",
    "    return path, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41454baa",
   "metadata": {},
   "source": [
    "### Farthest-First Traversal\n",
    "\n",
    "**PseudoCode:**\\\n",
    "The farthest-first traversal of a finite point set may be computed by a greedy algorithm that maintains the distance of each point from the previously selected points, performing the following steps:[3]\n",
    "\n",
    "    Initialize the sequence of selected points to the empty sequence, and the distances of each point to the selected points to infinity.\n",
    "    While not all points have been selected, repeat the following steps:\n",
    "        Scan the list of not-yet-selected points to find a point p that has maximum distance from the selected points.\n",
    "        Remove p from the not-yet-selected points and add it to the end of the sequence of selected points.\n",
    "        For each remaining not-yet-selected point q, replace the distance stored for q by the minimum of its old value and the distance from p to q.\n",
    "\n",
    "**Time complexity:** Worst Case O(N^2)\n",
    "\n",
    "**Space complexity:** Worst Case O(N)\n",
    "\n",
    "[Source (wikipedia)](https://en.wikipedia.org/wiki/Farthest-first_traversal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7378fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farthest-First Traversal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
